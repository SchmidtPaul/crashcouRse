---
title: "Von p-Werten und Signifikanzen"
---

> Dieses Kapitel ist zwar weniger ernst geschrieben als die anderen, ist aber mindestens genau so wichtig!
  
Das Thema, das in diesem Kapitel diskutiert wird, hat gerade in den letzten Jahren unter Statistikern zu [einer aufgeheizten Debatte](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#.XInvWyPMzyW) gef√ºhrt. Tats√§chlich ist das nicht korrekt formuliert, da alle Statistiker sich zu dem Thema sehr einig sind, ihnen aber das Herz blutet, wenn sie sehen wie p-Werte und Signifikanzen aus statistischen Analysen fehlinterpretiert oder gar manipuliert werden. Deshalb w√ºrde ich als Einstieg gerne die [Formulierung von Jenny Brian](https://www.tidyverse.org/articles/2017/12/workflow-vs-script/) entsprechend abwandeln und sagen:

<center>
**Wenn es dir nur darum geht, dass deine Ergebnisse signifikant sind, </br>
weil du ja sonst "nichts gefunden" hast, </br>
dann komm ich in dein B√ºro und Z√úNDE DEINEN COMPUTER AN** üî•
</center>

# Damals: Die Idee
Die Geschichte des p-Wertes geht [bis ins 17.Jh.](https://www.wikiwand.com/en/P-value#/History) zur√ºck. Es war schlie√ülich R.A. Fisher, der in seinen B√ºchern  [Statistical Methods for Research Workers (1925)](https://www.wikiwand.com/en/Statistical_Methods_for_Research_Workers) und [The Design of Experiments (1935)](https://www.wikiwand.com/en/The_Design_of_Experiments) den p-Wert nicht nur popul√§rer gemacht, sondern auch das heute g√§ngige Signifikanzniveau `p=0.05` vorgeschlagen hat. Bis dahin gab es auch noch kein Problem, da Fisher den p-Wert bzw. die Grenze 0.05 als Hilfsmittel angesehen hat - so wie jede andere statistische Ma√üzahl.

# Heute: Das Streben nach Signifikanz
Wissenschaft ist anstrengend. Viel M√ºhe und Zeit flie√üt in die Planung, Durchf√ºhrung und Auswertung von Experimenten. Wenn das Experiment dann noch durch eine Vermutung/Hypothese motiviert ist wie *"Mittel A f√ºhrt bestimmt zu h√∂heren Werten als Mittel B"*, dann ist von Anfang an eine Erwartungshaltung da: Der Unterschied soll gefunden und wissenschaftliche belegt werden. Es w√§re ja auch peinlich eine Hypothese aufzustellen, die am Ende gar nicht stimmt, oder?

Oft werden jungen Wissenschaftlern in diesem Kontext schon fr√ºh Worte wie "statistisch signifikant" zusammen mit positiven Gef√ºhlen vermittelt oder die `0.05` als magische Grenze daf√ºr ob ein Ergebnis etwas wert ist, bzw. ein Versuch √ºberhaupt was gebracht hat. In gewisser Hinsicht kann dies dem einzelnen √§lteren Wissenschaftler/Betreuer auch nicht ver√ºbelt werden, da wissenschaftliche Artikel mit statistisch signifikantem Ergebnis es heutzutage eher zur Publikation schaffen, als gleichwertige Artikel ohne statistische Signifikanz. Im Umkehrschluss f√ºhrt das aber dazu, dass ggf. zwar mehr Experimente zu einer Hypothese keinen (signifikanten) Unterschied gefunden haben, aber nur die wenigen Resultate ver√∂ffentlicht werden, in denen es doch zu (signifikanten) Unterschieden kam. Dieses Problem nennt man den [Publication Bias](https://www.wikiwand.com/de/Publikationsbias) und es verzerrt nat√ºrlich den Blick auf die Wahrheit. 

<center>
**Tats√§chlich kann ein p-Wert durchaus gewisse zus√§tzliche Informationen liefern, </br>
er wird aber leider oft √ºberbewertet, missverstanden und/oder missbraucht.**
</center>
</br>
 
# Was ist der p-Wert wirklich?
Der p-Wert (*p* f√ºr *probability*) wird auch *√úberschreitungswahrscheinlichkeit*, *Signifikanzwert* genannt und ist wie folgt definiert: 

</br><center>
**Der p-Wert ist die Wahrscheinlichkeit daf√ºr, </br> 
dass man das vorliegende oder ein noch extremeres Ergebnis findet, </br>
gegeben dem Fall, dass die Nullhypothese wahr ist.
**
</center></br> 

Der Satz ist zwar kompliziert, aber viel kompakter kann man ihn m.E. nicht schreiben. Wollen wir ihn also genauer betrachten:

+ Der p-Wert ist eine Wahrscheinlichkeit, kann also nur **Werte zwischen 0 und 1** (bzw. 0% und 100%) annehmen.
+ Der p-Wert **geht davon aus, dass die Nullhypothese wahr ist**. 
  + *Zur Erinnerung:* Wenn man einen Test durchf√ºhrt, stellt man vorher eine zu testende Nullhypothese ($H_0$) und die entsprechende Alternativhypothese ($H_A$) auf. Die Nullhypothese h√§ngt vom jeweiligen Test ab, besagt aber in der Regel, dass es keinen Effekt/Unterschied gibt. Bei einem t-test zum Mittelwertvergleich zweier Stichproben z.B. besagt sie, dass beide Mittelwerte gleich sind $H_0: \mu_1=\mu_2$ w√§hrend die Alternativhypothese sagt, dass die beiden Mittelwerte nicht gleich sind $H_A: \mu_1\neq\mu_2$. 
  + Der p-Wert geht demzufolge davon aus, dass $H_0$ wahr und es somit keinen Effekt/Unterschied gibt. 
+ Der p-Wert zeigt wie wahrscheinlich es ist **die vorhandenen Ergebnisse oder noch extremere Ergebnisse** zu finden.
  + "Noch extremer" meint dabei *noch mehr der Nullhypothese widersprechend*. Deuten die Ergebnisse also auf einen Unterschied hin, ist der p-Wert nicht nur die Wahrscheinlichkeit genau diesen Unterschied zu finden, sondern auch alle m√∂glichen noch gr√∂√üeren Unterschiede.

Demnach gilt, dass je kleiner ein p-Wert, desto mehr widersprechen die Ergebnisse der Nullhypothese. Da die Nullhypothese meist behauptet, dass es keinen Effekt gibt, bedeutet es dementsprechend in diesen F√§llen auch: Je kleiner der p-Wert, desto mehr widersprechen die Ergebnisse der Behauptung, dass es in Wirklichkeit keinen Effekt gibt. 

Au√üerdem kann man den p-Wert auch so betrachten: Sagen wir, wir erhalten in einem Test `p=0.04`, also 4%. Das bedeutet, dass wenn wir das Experiment 100 mal durchf√ºhren w√ºrden und - gegeben der Nullhypothese (=dass es keinen Effekt/Unterschied gibt) - nur 4 mal ein solches, oder ein noch extremeres Ergebnis finden w√ºrden. Wir haben das Experiment nat√ºrlich nur 1 mal durchgef√ºhrt.

### Ein Beispiel
Sagen wir die Zwillinge Arne und Tim sind exakt gleich gebaut und exakt gleich schnell im 100-m-Lauf: sie brauchen beide im Schnitt genau 11 Sekunden. Nichtsdestotrotz treten beide jeden Tag gegeneinander an um zu sehen wer schneller ist und notieren die Zeiten. Nach ein paar Tagen nehmen sie ihre Stichprobe und f√ºhren einen t-test durch um zu vergleichen ob es einen signifikanten Unterschied zwischen ihren Durchschnittszeiten gibt. Die Nullhypothese besagt, dass es keinen gibt $H_0: \mu_{Arne}=\mu_{Tim}$, w√§hrend die Alternativhypothese das Gegenteil behauptet $H_A: \mu_{Arne}\neq\mu_{Tim}$ (Der Alternativhypothese ist demnach auch egal wer von beiden schneller ist).

In diesem fiktiven Beispiel kennen wir die absolute Wahrheit, n√§mlich dass beide Mittelwerte gleich sind: $\mu_{Arne}=\mu_{Tim}=11,0s$. Das entspricht also der Nullhypothese. Aufgrund der jeweiligen Tagesform der beiden, aber auch der Messungenauigkeit der Stoppuhr, wird es allerdings niemals der Fall sein, dass alle gemessenen Zeit genau 11,0s sind. Stattdessen werden die Zeiten leicht um 11,0s schwanken. Demnach werden auch die Stichprobenmittelwerte ($\bar{x}_{Arne}$ und $\bar{x}_{Tim}$) so gut wie nie genau 11,0s sein, sondern vielleicht $\bar{x}_{Arne}=11,002s$ und $\bar{x}_{Tim}=10,996s$ f√ºr Tim. Und das obwohl wir hier sogar wissen, dass Arne und Tim in Wahrheit exakt gleich schnell sind.

W√ºrde man nun also schlie√üen, dass Tim wirklich ein schnellerer L√§ufer ist als Arne? Wie w√§re es bei $\bar{x}_{Arne}=11,00000000000001s$ und $\bar{x}_{Tim}=10,9999999999999999s$? Oder wie w√§re es bei $\bar{x}_{Arne}=12,0s$ und $\bar{x}_{Tim}=10,0s$? Die Entscheidung wo hier die Grenze gezogen werden soll ist schwer zu treffen und vor allem subjektiv. Genau hier soll der p-Wert helfen - Die Betonung liegt auf helfen und nicht komplett die Entscheidung abnehmen.

Da wir in diesem Beispiel nun *wissen*, dass f√ºr die wahren Mittelwerte gilt $\mu_{Arne}=\mu_{Tim}=11,0s$, w√§re es schon sehr eigenartig, wenn in unseren Stichproben rauskommt, dass $\bar{x}_{Arne}=12,0s$ und $\bar{x}_{Tim}=10,0s$. Tats√§chlich w√§re es nicht nur eigenartig, es w√§re schlichtweg unwahrscheinlich: der p-Wert f√ºr solch einen Fall w√§re klein. Noch extremer: Die Wahrscheinlichkeit daf√ºr, dass Arne beispielsweise sogar 100s und Tim nur 1s ben√∂tigt, geht demnach gegen 0% und der entsprechende p-Wert also auch.

### Wieso 0.05?
Nun ist also klar: Ein kleiner p-Wert bedeutet, dass es unwahrscheinlich ist das vorliegende Ergebnis zu finden, wenn doch eigentlich angeblich die Nullhypothese gilt. Im Umkehrschluss kann man sich also bei einem *zu kleinen* p-Wert entscheiden der Nullhypothese nicht l√§nger zu glauben. Genau das passiert, wenn der p-Wert kleiner 0.05 (5%) ist und man das Ergebnis als *statistisch signifikant* einstuft. Wieso sich diese Grenze nun genau bei 0.05 eingependelt hat ist nicht unbedingt klar. Sie gilt mittlerweile schlichtweg als g√§ngig und ist in den meisten Statistikprogrammen als *default* eingestellt. Man kann aber selbstverst√§ndlich auch manuell eine andere Grenze setzen wie z.B. `p=0.001` und das wird ab und an auch getan. 

### Der p-Wert h√§ngt vom Stichprobenumfang ab!
Es ist ein wichtiger erster Schritt zu verstehen was ein p-Wert genau bedeutet um Ergebnisse richtig einordnen zu k√∂nnen. Genau so wichtig ist aber auch zu verstehen wovon der p-Wert abh√§ngt. Das sind vor allem (aber nicht ausschlie√ülich):

+ die Stichprobengr√∂√üe bzw. Datenmenge
+ wie sehr die Daten aus unerkl√§rlichen Gr√ºnden streuen (= Noise)

Zur Erkl√§rung dieser beiden Punkte k√∂nnen wir wieder auf das Beispiel oben verweisen. Wieder gegeben der Nullhypothese, dass Arne und Tim gleich schnell sind ist es unwahrscheinlich, dass wir Stichprobenmittelwerte $\bar{x}_{Arne}=12,0s$ und $\bar{x}_{Tim}=10,0s$ finden. Man kann sich aber schnell vorstellen, dass dies in Ausnahmef√§llen mal passiert, wenn die beiden z.B. nur an zwei Tagen gelaufen sind und aus bestimmten Gr√ºnden es eben zu diesen Werten kam. Das waren dann eben zwei verr√ºckte Tage f√ºr diese Messungen. Es wird aber sehr schnell sehr viel unwahrscheinlicher diese Stichprobenmittelwerte zu finden, wenn die beiden 100 Tage gemessen haben. Genau so funktioniert auch der p-Wert: Bei `n=2` wir der p-Wert zwar klein sein, da wir Werte von 11,0s erwarten, aber bei `n=100` wird er gegen 0 gehen. Wir w√ºrden die Nullhypothese ablehnen und nicht l√§nger glauben, dass Arne und Tim gleich schnell sind - schlie√ülich sind sie ganze 100 mal gegeneinander angetreten und es liegen im Schnitt ganze 2 Sekunden zwischen ihren Zeiten.

Auch der zweite Punkt, n√§mlich die Streuung der Daten, beeinflusst den p-Wert. W√ºrden Arnes und Timis Zeiten immer nur 0,1s von ihrem Stichprobenmittelwert abweichen, w√§re es schlie√ülich auch viel deutlicher zu sagen wer schneller ist, als wenn beide immer mal 5s schneller oder langsamer sind. 

### Ein Test kann nichts au√üer die Nullhypothese ablehnen!
Aus dem vorangegangen Abschnitt ergibt sich eine weitere Interpretationsweise des p-Werts:

</br><center>
**Der p-Wert dr√ºckt (indirekt) aus wie viel Evidenz wir haben </br> 
um die Nullhypothese abzulehnen.**
</center></br> 

Je kleiner der p-Wert, desto sicherer sind wir uns, dass die Nullhypothese nicht stimmt. Wichtig ist, dass dies auch tats√§chlich die einzige Entscheidung ist, die wir bei einem Test treffen k√∂nnen. Wenn der p-Wert gr√∂√üer als 0.05 ist und demnach nicht signifikant, dann lehnen wir die Nullhypothese nicht ab. 

</br><center>
**Die Nullhypothese nicht ablehnen zu k√∂nnen (p>0.05) bedeutet nicht unbedingt, dass die Nullhypothese wahr ist!**
</center></br> 

Stattdessen kann es zwei Gr√ºnde geben warum man die Nullhypothese nicht ablehnen konnte:

1. Die Nullhypothese ist tats√§chlich nicht wahr.
2. Wir hatte nicht genug Evidenz (z.B. zu kleine Stichprobengr√∂√üe) um die Nullhypothese abzulehnen.

Wenn man wirklich testen m√∂chte ob z.B. zwei Mittelwerte gleich sind, dann k√∂nnte man einen √Ñquivalenztest anstelle eines t-tests durchf√ºhren. Mehr zum Thema z.B. [hier](https://en.wikivet.net/Hypothesis_testing)

# Signifikanz $\neq$ Relevanz
Wenn man das alles mal sacken l√§sst und ein Gef√ºhl daf√ºr bekommt was ein p-Wert nun wirklich ausdr√ºcken kann, was er nicht ausdr√ºcken kann und wie man ihn durch z.B die Stichprobengr√∂√üe beeinflussen kann, dann wird es Zeit f√ºr eine Anekdote. Was jetzt folgt ist wirklich passiert und wohl auch der Grund warum ich hier ab und an von brennenden Computern üî• spreche:

Eine Doktorandin aus der biologischen Fakult√§t kommt f√ºr eine statistische Beratung zu uns, da sie demn√§chst ein Experiment durchf√ºhren m√∂chte um zu pr√ºfen ob ein Mittel zu einem erh√∂hten Wachstum bei Pflanzen f√ºhrt. Sie will demnach einigen Pflanzen das Mittel verarbreichen und anderen nicht und nach einer bestimmten Zeit messen wie hoch die Pflanzen gewachsen sind.</br>
**Doktorandin:** "Wie viele Wiederholungen sollte ich machen um Unterschiede zu finden?"</br>
**Wir:** "Das kommt darauf an was f√ºr Unterschiede du finden m√∂chtest."</br>
**Doktorandin:** "Na signifikante Unterschiede nat√ºrlich!"</br>
**Wir:** "Naja, statistisch signifikant kriegen wie jeden Unterschied, wenn wir nur genug Wiederholungen machen. Ich meinte eher wie viel mm oder cm Unterschied in der Pflanzenh√∂he f√ºr dich ein echter, also biologisch relevanter Unterschied w√§re. Das m√ºsst ihr als Experten auf dem Gebiet festlegen, damit die Statistik helfen kann diesen zu finden."</br>
**Doktorandin:** "Achso? Dar√ºber habe ich noch nicht nachgedacht - ich frage mal meinen Professor."</br>
*[Treffen vorbei, ein paar Tage kein Kontakt.]*</br>
**Email Doktorandin:** "Hallo Paul! Ich habe noch keine Antwort auf deine Frage ab wann genau es f√ºr uns ein echter Unterschied w√§re, aber mein Professor hat gesagt, dass wie vier Wiederholungen machen werden, weil wir das immer so machen. Danke nochmal!"

Diese Konversation fasst f√ºr mich bis heute sehr gut zusammen was *Statistische Signifikanz* $\neq$ *Biologische Relevanz* ausdr√ºcken soll. Der urspr√ºnglich als Hilfsmittel gedachte p-Wert wird missbraucht indem er die eigentliche Fachexpertise v√∂llig ersetzt. Mehr zum Thema beispielsweise [hier](https://efsa.onlinelibrary.wiley.com/doi/pdf/10.2903/j.efsa.2011.2372) und [hier](https://pubs.acs.org/doi/pdf/10.1021/jf401124y)


# Fehlinterpretationen des p-Werts
Achtung, hier tauchen ein paar **falsche** Aussagen √ºber den p-Wert auf. Bitte nicht im Kopf durcheinanderbringen mit der korrekten Interpretation. Falls du in Zukunft eine der folgenden Aussagen in deine Arbeiten schreibst, dann komm ich in dein B√ºro und Z√úNDE DEINEN COMPUTER AN üî•


<span style="color:red"> **FALSCH:** Wenn `p=0.05`, dann ist die Chance, dass die Nullhypothese wahr ist, nur 5%. </span> 
</br>
<span style="color:ForestGreen"> **RICHTIG:** Der p-Wert geht sowieso immer davon aus, dass die Nullhypothese stimmt. </span> 


<span style="color:red"> **FALSCH:** Ein nicht-signifikanter Unterschied bedeutet, dass die Mittelwerte gleich sind oder es keinen Effekt gibt. </span> 
</br>
<span style="color:ForestGreen"> **RICHTIG:** Die Nullhypothese nicht ablehnen zu k√∂nnen bedeutet nicht unbedingt, dass die Nullhypothese wahr ist. </span> 


<span style="color:red"> **FALSCH:** Nur ein signifikanter Unterschied bedeutet, dass das Ergebnis in der
Realit√§t wichtig ist. </span> 
</br>
<span style="color:ForestGreen"> **RICHTIG:** Statistische Signifikanz ist nicht gleichzusetzen mit biologischer Relevanz. </span> 

Weitere Falschaussagen mit Korrekturen finden sich z.B. [hier](http://www.biometrische-gesellschaft.de/fileadmin/AG_Daten/Landwirtschaft/PDFs/Tuchscherer_Vortrag_2019.pdf). 

> **Mehr zum Thema:** </br>
[Wikipedia: Misuse of p-values](https://www.wikiwand.com/en/Misuse_of_p-values) </br>
[Wikipedia: p-Hacking](https://www.wikiwand.com/de/P-Hacking) </br>
[Most relevant Youtube Videos: p-hacking](https://www.youtube.com/results?search_query=p+hacking) </br>
[The ASA Statement on p-Values: Context, Process and Purpose (2016)](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#.XWZIPegzaUn) </br>
[Schmidt et al. (2016): Enhancing the interpretation of statistical P values in toxicology studies](https://link.springer.com/article/10.1007/s00204-015-1487-8) </br>
[Nature (2019): It‚Äôs time to talk about ditching statistical significance](https://www.nature.com/articles/d41586-019-00874-8) </br>
[Nature (2019): Scientists rise up against statistical significance](https://www.nature.com/articles/d41586-019-00857-9) </br>
[S√ºddeutsche (2019): Signifikanter Unfug](https://www.sueddeutsche.de/wissen/statistik-p-wert-signifikanz-hypothese-nullhypothese-1.4375636)




